{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "xy_train_df = pd.read_csv('xy_train.csv')\n",
    "x_test_df = pd.read_csv('x_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(48000, 40) (48000,)\n",
      "(12000, 40) (12000,)\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pprint import pprint\n",
    "\n",
    "vocab_size = 40000\n",
    "max_len = 40\n",
    "\n",
    "\n",
    "x = xy_train_df.text\n",
    "y = xy_train_df.label\n",
    "\n",
    "x_train, x_valid, y_train, y_valid = train_test_split(x, y, test_size=0.2)\n",
    "\n",
    "# build vocabulary from training set\n",
    "tokenizer = Tokenizer(num_words=vocab_size)\n",
    "tokenizer.fit_on_texts(x_train)\n",
    "\n",
    "\n",
    "def _preprocess(list_of_text):\n",
    "    return pad_sequences(\n",
    "        tokenizer.texts_to_sequences(list_of_text),\n",
    "        maxlen=max_len,\n",
    "        padding='post',\n",
    "    )\n",
    "    \n",
    "\n",
    "# padding is done inside: \n",
    "x_train = _preprocess(x_train)\n",
    "x_valid = _preprocess(x_valid)\n",
    "\n",
    "print(x_train.shape, y_train.shape)\n",
    "print(x_valid.shape, y_valid.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"a smart border wall would destroy historic gravesites in south texas is 'no \"\n",
      " \"longer a reliably red state'\",\n",
      " 'my uncle on a secret mission in vietnam to rescue forgotten pows he passed '\n",
      " 'away in 1996 from agent orange and we had to fight the va to get his records '\n",
      " 'and this picture released',\n",
      " 'colorized violence erupts as drug dealer harasses youth on the streets of '\n",
      " 'san francisco 1982',\n",
      " 'escaped nazi genetic experiment found wandering on the banks of the rhine c '\n",
      " '1943 colorized',\n",
      " 'canadian ambassador says nafta deal reached in 1992 cbc report on toronto '\n",
      " 'lawyer toronto star']\n"
     ]
    }
   ],
   "source": [
    "pprint(tokenizer.sequences_to_texts(x_train[:5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total words in the dictionary: 40000\n"
     ]
    }
   ],
   "source": [
    "print('total words in the dictionary:', tokenizer.num_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "750/750 [==============================] - 16s 22ms/step - loss: 0.5594 - accuracy: 0.7296 - auc: 0.8195 - val_loss: 0.4562 - val_accuracy: 0.7937 - val_auc: 0.8743\n",
      "Epoch 2/20\n",
      "750/750 [==============================] - 16s 21ms/step - loss: 0.3829 - accuracy: 0.8343 - auc: 0.9137 - val_loss: 0.4146 - val_accuracy: 0.8075 - val_auc: 0.8915\n",
      "Epoch 3/20\n",
      "750/750 [==============================] - 16s 21ms/step - loss: 0.3119 - accuracy: 0.8673 - auc: 0.9421 - val_loss: 0.4140 - val_accuracy: 0.8100 - val_auc: 0.8937\n",
      "Epoch 4/20\n",
      "750/750 [==============================] - 15s 20ms/step - loss: 0.2638 - accuracy: 0.8891 - auc: 0.9574 - val_loss: 0.4297 - val_accuracy: 0.8069 - val_auc: 0.8913\n",
      "Epoch 5/20\n",
      "750/750 [==============================] - 15s 21ms/step - loss: 0.2251 - accuracy: 0.9047 - auc: 0.9677 - val_loss: 0.4537 - val_accuracy: 0.8048 - val_auc: 0.8881\n",
      "Epoch 6/20\n",
      "750/750 [==============================] - 15s 20ms/step - loss: 0.1928 - accuracy: 0.9169 - auc: 0.9748 - val_loss: 0.4864 - val_accuracy: 0.8007 - val_auc: 0.8832\n",
      "Epoch 7/20\n",
      "750/750 [==============================] - 15s 20ms/step - loss: 0.1658 - accuracy: 0.9260 - auc: 0.9795 - val_loss: 0.5262 - val_accuracy: 0.7913 - val_auc: 0.8770\n",
      "Epoch 8/20\n",
      "750/750 [==============================] - 15s 20ms/step - loss: 0.1413 - accuracy: 0.9342 - auc: 0.9834 - val_loss: 0.5721 - val_accuracy: 0.7898 - val_auc: 0.8719\n",
      "Epoch 9/20\n",
      "750/750 [==============================] - 15s 20ms/step - loss: 0.1187 - accuracy: 0.9413 - auc: 0.9863 - val_loss: 0.6240 - val_accuracy: 0.7811 - val_auc: 0.8654\n",
      "Epoch 10/20\n",
      "750/750 [==============================] - 15s 20ms/step - loss: 0.0978 - accuracy: 0.9466 - auc: 0.9884 - val_loss: 0.6805 - val_accuracy: 0.7778 - val_auc: 0.8594\n",
      "Epoch 11/20\n",
      "750/750 [==============================] - 15s 20ms/step - loss: 0.0792 - accuracy: 0.9510 - auc: 0.9899 - val_loss: 0.7461 - val_accuracy: 0.7740 - val_auc: 0.8537\n",
      "Epoch 12/20\n",
      "750/750 [==============================] - 15s 20ms/step - loss: 0.0601 - accuracy: 0.9553 - auc: 0.9914 - val_loss: 0.8155 - val_accuracy: 0.7687 - val_auc: 0.8472\n",
      "Epoch 13/20\n",
      "750/750 [==============================] - 15s 20ms/step - loss: 0.0426 - accuracy: 0.9578 - auc: 0.9923 - val_loss: 0.8877 - val_accuracy: 0.7692 - val_auc: 0.8423\n",
      "Epoch 14/20\n",
      "750/750 [==============================] - 15s 20ms/step - loss: 0.0236 - accuracy: 0.9609 - auc: 0.9932 - val_loss: 0.9800 - val_accuracy: 0.7663 - val_auc: 0.8375\n",
      "Epoch 15/20\n",
      "750/750 [==============================] - 15s 20ms/step - loss: 0.0062 - accuracy: 0.9628 - auc: 0.9938 - val_loss: 1.0585 - val_accuracy: 0.7624 - val_auc: 0.8297\n",
      "Epoch 16/20\n",
      "750/750 [==============================] - 15s 20ms/step - loss: -0.0122 - accuracy: 0.9648 - auc: 0.9944 - val_loss: 1.1655 - val_accuracy: 0.7530 - val_auc: 0.8226\n",
      "Epoch 17/20\n",
      "750/750 [==============================] - 15s 20ms/step - loss: -0.0302 - accuracy: 0.9661 - auc: 0.9947 - val_loss: 1.2747 - val_accuracy: 0.7507 - val_auc: 0.8175\n",
      "Epoch 18/20\n",
      "750/750 [==============================] - 15s 20ms/step - loss: -0.0496 - accuracy: 0.9690 - auc: 0.9953 - val_loss: 1.3686 - val_accuracy: 0.7511 - val_auc: 0.8136\n",
      "Epoch 19/20\n",
      "750/750 [==============================] - 15s 21ms/step - loss: -0.0688 - accuracy: 0.9696 - auc: 0.9954 - val_loss: 1.4869 - val_accuracy: 0.7475 - val_auc: 0.8078\n",
      "Epoch 20/20\n",
      "750/750 [==============================] - 15s 20ms/step - loss: -0.0881 - accuracy: 0.9718 - auc: 0.9956 - val_loss: 1.6044 - val_accuracy: 0.7452 - val_auc: 0.8046\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "import collections\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.compat.v1.keras.layers import CuDNNGRU\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "seq_in = keras.Input(batch_shape=(None, max_len))\n",
    "embedded = keras.layers.Embedding(tokenizer.num_words, 100)(seq_in)\n",
    "averaged = tf.reduce_mean(embedded, axis=1)\n",
    "pred = keras.layers.Dense(1, activation='sigmoid')(averaged)\n",
    "\n",
    "model = keras.Model(\n",
    "    inputs=seq_in,\n",
    "    outputs=pred,\n",
    ")\n",
    "\n",
    "model.compile(\n",
    "    optimizer=Adam(),\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy', 'AUC']\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "history = model.fit(x_train,\n",
    "                    y_train,\n",
    "                    epochs=20,\n",
    "                    batch_size=64,\n",
    "                    validation_data=(x_valid, y_valid),\n",
    "                    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = _preprocess(x_test_df.text)\n",
    "y_predict = np.squeeze(model.predict(x_test))\n",
    "\n",
    "\n",
    "pd.DataFrame(\n",
    "    {'id': x_test_df.index,\n",
    "     'label':y_predict}).to_csv('sample_submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
